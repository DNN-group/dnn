{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise sheet 2: Training and Regularization\n",
    "\n",
    "In this homework, we will train neural networks on the Boston housing dataset. For this, we will use the modular framework developed in Homework 1. A first part of the homework will analyze the parameters of the network before and after training. A second part of the homework will test some regularization penalties and their effect on the generalization error.\n",
    "\n",
    "## Boston Housing Dataset\n",
    "\n",
    "The following code extracts the Boston housing dataset in a way that is already partitioned into training and test data. The data is normalized such that each dimension has mean 0 and variance 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "Xtrain,Ttrain,Xtest,Ttest = utils.getBostonHousingData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Regressor\n",
    "\n",
    "In this homework, we will consider a very simple architecture consisting of one linear layer, a ReLU layer applying a nonlinear function element-wise, and a pooling layer computing a fixed weighted sum of the activations obtained in the previous layer. The architecture is shown below:\n",
    "\n",
    "![Diagram of the Neural Network Regressor used in this homework](neuralnet.png)\n",
    "\n",
    "The function `getarch` shown below generates an architecture specific to the Boston housing dataset, with 13 input features, `h` intermediate neurons, and one output. It takes as a parameter the first layer type, which is usually `layers.Linear`. Later in this homework, we will replace it by variants of `layers.Linear` that incorporate weight norm penalties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import layers\n",
    "\n",
    "def getarch(FirstLayerType):\n",
    "    h = 100\n",
    "    return layers.Sequential([\n",
    "        FirstLayerType(13,h,seed=0),\n",
    "        layers.ReLU(),\n",
    "        layers.Pooling(h)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `NeuralNetworkRegressor` shown below takes a neural network architecture, trains it on the data using gradient descent with momentum, and predict new data points. Because the dataset is rather small, we do not need to use stochastic gradient descent. We choose instead the batch variant, where we follow the gradient of the true mean square error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn,sklearn.metrics\n",
    "\n",
    "class NeuralNetworkRegressor:\n",
    "\n",
    "    def __init__(self,arch): self.arch = arch\n",
    "\n",
    "    def fit(self,X,T,nbit=10000):\n",
    "\n",
    "        for i in range(nbit):\n",
    "            self.arch.backward(2*(self.arch.forward(X)-T))\n",
    "            for l in self.arch.layers: l.computepgrad(); l.pgradstep(0.01)\n",
    "                \n",
    "    def predict(self,X):\n",
    "        return self.arch.forward(X)\n",
    "\n",
    "    def score(self,X,T):\n",
    "        return sklearn.metrics.r2_score(T,self.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Performance vs. Baselines\n",
    "\n",
    "We compare the performance of the neural network on the Boston housing data to two other regressors: a random forest and a support vector regression model with RBF kernel. We use the scikit-learn implementation of these models, with their default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.ensemble,sklearn.svm\n",
    "\n",
    "rfr = sklearn.ensemble.RandomForestRegressor()\n",
    "rfr.fit(Xtrain,Ttrain)\n",
    "\n",
    "svr = sklearn.svm.SVR()\n",
    "svr.fit(Xtrain,Ttrain)\n",
    "\n",
    "nnr = NeuralNetworkRegressor(getarch(layers.Linear))\n",
    "nnr.fit(Xtrain,Ttrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">    RForest | R2train:  0.953 | R2test:  0.820\n",
      ">        SVR | R2train:  0.913 | R2test:  0.758\n",
      ">      NNreg | R2train:  0.996 | R2test:  0.845\n"
     ]
    }
   ],
   "source": [
    "def pretty(name,model):\n",
    "    return '> %10s | R2train: %6.3f | R2test: %6.3f'%(name,model.score(Xtrain,Ttrain),model.score(Xtest,Ttest))\n",
    "\n",
    "print(pretty('RForest',rfr))\n",
    "print(pretty('SVR',svr))\n",
    "print(pretty('NNreg',nnr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural networks performs on par with the other regression models although it has in principle the added ability to learn its own internal features. We would therefore expect a well-trained neural network to perform better.\n",
    "\n",
    "## Gradient, and Parameter Norms (20 P)\n",
    "\n",
    "As a first step towards improving the neural network model, we will measure proxy quantities, that will then be used to regularize the model. We consider the following three quantities:\n",
    "\n",
    " * $\\|W\\|_\\text{Frob} =  \\sqrt{\\sum_{i=1}^d \\sum_{j=1}^h  w_{ij}^2}$\n",
    " * $\\|W\\|_\\text{Mix} = h^{-0.5} \\sqrt{\\sum_{i=1}^d \\big(\\sum_{j=1}^h | w_{ij}|\\big)^2}$\n",
    " * $\\text{Grad} = \\textstyle \\frac1N \\sum_{n=1}^N\\|\\nabla_{\\boldsymbol{x}}f (\\boldsymbol{x_n})\\|$\n",
    "\n",
    "where $d$ is the number of input features, $h$ is the number of neurons in the hidden layer, and $W$ is the matrix of weights in the first layer. In order for the model to generalize well, the last quantity ($\\text{Grad}$) should be prevented from becoming too large. Because the latter depends on the data distribution, we rely instead on the inequality $\\text{Grad} \\leq \\|W\\|_\\text{Mix} \\leq \\|W\\|_\\text{Frob}$, that we can prove for this model, and will try to control the weight norms instead.\n",
    "\n",
    "#### Tasks:\n",
    "\n",
    "* Implement the function `WMix(nn)` that receives the neural network as input and returns $\\|W\\|_\\text{Mix}$.\n",
    "* Implement the function `Grad(nn,X)` that receives the neural network and some dataset as input, and computes the averaged gradient norm ($\\text{Grad}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def WFrob(nn):\n",
    "    W = nn.arch.layers[0].W\n",
    "    return (W**2).sum()**.5\n",
    "\n",
    "def WMix(nn):\n",
    "    W = nn.arch.layers[0].W\n",
    "    return (1 / W.shape[1] * (abs(W).sum(axis = 1) ** 2).sum())**0.5\n",
    "import numpy as np\n",
    "\n",
    "def Grad(nn,X):\n",
    "    W = nn.arch.layers[0].W\n",
    "    B = nn.arch.layers[0].B\n",
    "    def grad1(X,W,B):\n",
    "        s = np.zeros(W.shape[0])\n",
    "        for j in range(W.shape[1]):\n",
    "            I = X.dot(W[:,j]) + B[j] > 0\n",
    "            s += W[:,j] * I\n",
    "        return (s**2).sum()**0.5\n",
    "    ret = np.mean([grad1(x,W,B) for x in X])\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following experiment computes these three quantities before and after training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">     Before | R2train: -0.043 | R2test: -0.006 | WFrob:   9.834 | WMix:   7.845 | Grad:  13.562\n",
      ">      After | R2train:  0.996 | R2test:  0.845 | WFrob:  18.752 | WMix:  15.018 | Grad:  26.402\n"
     ]
    }
   ],
   "source": [
    "global count \n",
    "count = 0\n",
    "def fullpretty(name,nn):\n",
    "    return pretty(name,nn) + ' | WFrob: %7.3f | WMix: %7.3f | Grad: %7.3f'%(WFrob(nn),WMix(nn),Grad(nn,Xtest))\n",
    "\n",
    "nnr = NeuralNetworkRegressor(getarch(layers.Linear))\n",
    "print(fullpretty('Before',nnr))\n",
    "nnr.fit(Xtrain,Ttrain)\n",
    "print(fullpretty('After',nnr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the inequality $\\text{Grad} \\leq \\|W\\|_\\text{Mix} < \\|W\\|_\\text{Frob}$ also holds empirically. We also observe that these quantities tend to increase as training proceeds. This is a typical behavior, as the network starts rather simple and becomes complex as more and more variations in the training data are being captured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frobenius-Norm Penalty (10 P)\n",
    "\n",
    "The first penalty we experiment with is the squared Frobenius norm. We consider the new objective $J^\\text{Frob}(\\theta) = \\text{MSE}(\\theta) + \\lambda \\cdot \\|W\\|_\\text{Frob}^2$, where the first term is the original mean square error objective and where the second term is the added penalty. We hardcode the penalty coeffecient to $\\lambda = 10^{-3}$. In principle, for maximum performance and fair comparison between the methods, several of them should be tried (also for other models), and selected based on some validation set. Here, for simplicity, we omit this step.\n",
    "\n",
    "#### Tasks:\n",
    "\n",
    "* Create a new layer, that replaces the original `layers.Linear` layer and incorporates the proposed penalty. *(Hint: This can be achieved by extending layers.Linear, and rewriting its method `computepgrad`.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# TODO: Replace by your code\n",
    "# ------------------------------------------\n",
    "# import solution\n",
    "# LinearFrob = solution.LinearFrob\n",
    "import copy\n",
    "LinearFrob = copy.deepcopy(layers.Linear)\n",
    "def computepgradFrob (self,lam = 1/10**3):\n",
    "    self.DW = np.dot(self.A.T,self.DZ)/len(self.A) + lam *2 * self.W\n",
    "    self.DB = self.DZ.mean(axis=0)\n",
    "LinearFrob.computepgrad = computepgradFrob\n",
    "\n",
    "# ------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below trains a neural network with the new first layer, and compares the performance with the previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nnfrob = NeuralNetworkRegressor(getarch(LinearFrob))\n",
    "nnfrob.fit(Xtrain,Ttrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">    RForest | R2train:  0.953 | R2test:  0.820\n",
      ">        SVR | R2train:  0.913 | R2test:  0.758\n",
      ">         NN | R2train:  0.996 | R2test:  0.845 | WFrob:  18.752 | WMix:  15.018 | Grad:  26.402\n",
      ">    NN+Frob | R2train:  0.967 | R2test:  0.833 | WFrob:   5.119 | WMix:   4.139 | Grad:   7.263\n"
     ]
    }
   ],
   "source": [
    "print(pretty('RForest',rfr))\n",
    "print(pretty('SVR',svr))\n",
    "print(fullpretty('NN',nnr))\n",
    "print(fullpretty('NN+Frob',nnfrob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the penalty has the effect of strongly reducing the Frobenius norm of the weight matrix. However, the test accuracy is not improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed-Norm Penalty (10 P)\n",
    "\n",
    "A downside of the Frobenius norm is that it is not a very tight upper bound of the gradient, that is, penalizing it is does not penalize specifically high gradient. Instead, other useful properties of the model could be negatively affected by it. In the following, we experiment with the mixed-norm regularizer, which is a tighter bound of the gradient.\n",
    "\n",
    "The objective function is now given by $\\textstyle J^{\\text{Mix}}(\\theta) = \\text{MSE}(\\theta) + \\lambda \\cdot \\|W\\|_\\text{Mix}^2$ , where the first and second terms are again the original objective and the penalty term. As for the previous exercise, we hardcode the latter coefficient to $10^{-3}$.\n",
    "\n",
    "#### Tasks:\n",
    "\n",
    "* Using the same technique as before, create a new layer that incorporates the Mixed norm penalty.\n",
    "\n",
    "The differential of the Mix norm for element $W_{l,k}$ is\n",
    "$\n",
    "    \\frac{\\partial\\|W\\|_\\text{Mix}^2}{\\partial W_{l,k}}\n",
    "        = 2h^{-1}\\sum_{i=1}^{h}|W_{l,j}|\\text{sign}(W_{l,k})\n",
    "        = 2h^{-1}\\|W_{l,:}\\|_1\\text{sign}(W_{l,k})\n",
    "$\n",
    "thus\n",
    "$\\frac{\\partial\\|W\\|_\\text{Mix}^2}{\\partial W}\n",
    "        = 2h^{-1}\\tilde{W}_{(1)} 1_h^T \\odot\\text{sign}(W)$\n",
    "with $\\tilde{W}_{(1)} := (\\|W_{1,:}\\|_1, ..., \\|W_{d,:}\\|_1)^T$. Reminder: $\\odot$ represents the operator for the elemente-wise matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LinearMix = copy.deepcopy(layers.Linear)\n",
    "def computepgradMix (self,lam = 1/10**3):\n",
    "    _W2 = np.outer(abs(self.W).sum(axis= 1),np.ones(self.W.shape[1]))\n",
    "    self.DW = np.dot(self.A.T,self.DZ)/len(self.A) + lam * 2 / self.W.shape[1] * np.multiply(_W2,np.sign(self.W))\n",
    "    self.DB = self.DZ.mean(axis=0)\n",
    "LinearMix.computepgrad = computepgradMix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below trains a neural network with the new penalty, and compares the performance with the previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nnmix = NeuralNetworkRegressor(getarch(LinearMix))\n",
    "nnmix.fit(Xtrain,Ttrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">    RForest | R2train:  0.953 | R2test:  0.820\n",
      ">        SVR | R2train:  0.913 | R2test:  0.758\n",
      ">         NN | R2train:  0.996 | R2test:  0.845 | WFrob:  18.752 | WMix:  15.018 | Grad:  26.402\n",
      ">    NN+Frob | R2train:  0.967 | R2test:  0.833 | WFrob:   5.119 | WMix:   4.139 | Grad:   7.263\n",
      ">     NN+Mix | R2train:  0.978 | R2test:  0.856 | WFrob:   8.188 | WMix:   4.214 | Grad:   8.821\n"
     ]
    }
   ],
   "source": [
    "print(pretty('RForest',rfr))\n",
    "print(pretty('SVR',svr))\n",
    "print(fullpretty('NN',nnr))\n",
    "print(fullpretty('NN+Frob',nnfrob))\n",
    "print(fullpretty('NN+Mix',nnmix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to observe that this mixed norm penalty more selectively reduced the mixed norm and the gradient, and has let the Frobenius norm take higher values. The mixed-norm model is also the one that produces the highest test set accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
