{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks: Forward and Backward Propagation\n",
    "\n",
    "In this homework, our goal is to test different approaches to program neural networks, from the simplest and least general one to the most complex and general. Here, we will be focusing on programming forward and gradient computations. Training neural networks will be left for Homework 2. The first implementation we consider is a hard-coded neural network made of one input layer, three hidden layers, and one output layer. We use the ReLU nonlinearity at each hidden layer. The neural network is depicted below:\n",
    "\n",
    "![](files/net.svg.png)\n",
    "\n",
    "The following implementation performs forward and gradient computations using **`for`**  loops. The code is compact but only works for this specific architecture. It is hard to extend it to new layers without significantly refactoring and complexifying the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy,numpy.linalg\n",
    "\n",
    "def implementation1(X,T,W,B):\n",
    "\n",
    "    # 1. Initialize some data structures\n",
    "    A,Z = [X],[]\n",
    "    DW,DB = [],[]\n",
    "\n",
    "    # 2. Run the forward pass\n",
    "    for i,(w,b) in enumerate(zip(W,B)):\n",
    "        Z.append(A[-1].dot(w)+b)\n",
    "        if i in [0,1,2]: A.append(numpy.maximum(0,Z[-1]))\n",
    "    Y = Z[-1]\n",
    "\n",
    "    # 3. Compute the error\n",
    "    Y = Z[-1]\n",
    "    err = ((Y-T)**2).mean()\n",
    "    grad = 2*(Y-T)\n",
    "\n",
    "    # 4. Gradient propagation\n",
    "    for w,b,a in zip(W[::-1],B[::-1],A[::-1]):\n",
    "        DW.insert(0,a.T.dot(grad)/len(a))\n",
    "        DB.insert(0,grad.mean(axis=0))\n",
    "        grad = grad.dot(w.T)*(a>0)\n",
    "\n",
    "    # 5. Return error and gradient\n",
    "    return err,DW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below applies this function to a given dataset `X,T` at a given position `W,B` in parameter space, where these two variables contain the weight and bias parameters at each layer. It then prints the prediction mean square error, and the gradient norm at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.1593903177325451, [0.29622485370120832, 0.6949589001193901, 1.3754967914824301, 0.80223630277879099])\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "X,T = utils.getdata(100)\n",
    "W,B = utils.getparameters([X.shape[1],10,15,10,T.shape[1]])\n",
    "\n",
    "err,DW = implementation1(X,T,W,B)\n",
    "\n",
    "print(err,map(numpy.linalg.norm,DW))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers are not specially interesting (the network has not been trained), however, they are useful for debugging purposes, in order to verify that the next implementations work as expected.\n",
    "\n",
    "## Object-Oriented Implementation (15 P)\n",
    "\n",
    "The following implmentation adopts an object oriented approach to the forward and gradient computations. Each layer is an object with methods implementing the forward and backward pass for this layer. Objects are defined in the file `layers.py`. Overall, the code is longer, but it is better structured and easier to extend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import layers\n",
    "    \n",
    "def implementation2(X,T,W,B,NonLin):\n",
    "    \n",
    "    # 1. Build the neural network\n",
    "    nnlayers = []\n",
    "    for w,b in zip(W,B): nnlayers += [layers.Linear(w,b)] + ([NonLin()] if len(b)!=1 else [])\n",
    "    nn = layers.Sequential(nnlayers)\n",
    "\n",
    "    # 2. Compute the error and its gradient\n",
    "    Y = nn.forward(X)\n",
    "    err = ((Y-T)**2).mean()\n",
    "    nn.backward(2*(Y-T))\n",
    "\n",
    "    # 3. Return them\n",
    "    return err,[nn.layers[i].DW for i in [0,2,4,6]]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below computes and prints the prediction error, and the gradient norm at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0609541173391692, [0.24642012799448015, 0.49094047912050492, 0.66230622343522649, 0.47314564399410192])\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "err,DW = implementation2(X,T,W,B,layers.Tanh)\n",
    "print(err,map(numpy.linalg.norm,DW))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, although the data and parameters have not been changed, the numbers are different as for `implementation1`, as we have now made use of the `Tanh` nonlinearity instead of `ReLU`.\n",
    "\n",
    "**Tasks:** \n",
    "\n",
    " * **Define a new layer `ReLU` to be used in replacement to the `Tanh` layer. This makes the architecture equivalent to the neural network implemented by the function `implementation1`.**\n",
    " * **Run the code below to verify that the error and gradient are indeed the same for `implementation1` and  `implementation2`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.1593903177325451, [0.29622485370120832, 0.6949589001193901, 1.3754967914824301, 0.80223630277879099])\n",
      "(1.1593903177325451, [0.29622485370120832, 0.6949589001193901, 1.3754967914824301, 0.80223630277879099])\n"
     ]
    }
   ],
   "source": [
    "class ReLU:\n",
    "    \n",
    "    def forward(self,Z): self.A = numpy.maximum(0,Z); return self.A\n",
    "    def backward(self,DA): return (self.A>0)*DA \n",
    "\n",
    "err,DW = implementation1(X,T,W,B);      print(err,map(numpy.linalg.norm,DW))\n",
    "err,DW = implementation2(X,T,W,B,ReLU); print(err,map(numpy.linalg.norm,DW))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation for General Graphs (15 P)\n",
    "\n",
    "The implementation below is more complex but is also applicable to a broader set of structures than simple feed-forward network. Here, the neural network can be seen as a set of nodes (defined in `nodes.py`) that are organized in a graph. Prediction and computation of gradients are obtained by traversing the graph using recursion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nodes\n",
    "    \n",
    "def implementation3(X,T,W,B):\n",
    "\n",
    "    # 1. Build the neural network\n",
    "    W,B = utils.getparameters([X.shape[1],10,15,10,T.shape[1]])\n",
    "\n",
    "    nodeX = nodes.Input()\n",
    "    nodeA = nodeX\n",
    "    nodesW = [nodes.Weight(w) for w in W]\n",
    "    nodesB = [nodes.Bias(b) for b in B]\n",
    "\n",
    "    for i,(nodeW,nodeB) in enumerate(zip(nodesW,nodesB)):\n",
    "        nodeZ = nodes.Linear(nodeA,nodeW,nodeB)\n",
    "        if i in [0,1,2]: nodeA = nodes.Tanh(nodeZ)\n",
    "    nodeY = nodes.Output(nodeZ)\n",
    "\n",
    "    # 2. Compute the error and its gradient\n",
    "    nodeX.feed(X)\n",
    "    Y = nodeY.evaluate()\n",
    "    err = ((Y-T)**2).mean()\n",
    "    nodeY.feed(2*(Y-T))\n",
    "    \n",
    "    # 3. Return them\n",
    "    return err,[nodeW.grad() for nodeW in nodesW]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below applies the new implementation on the same dataset and parameters as before, and the error and gradients are compared for correctness to those of `implementation2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0609541173391692, [0.24642012799448015, 0.49094047912050492, 0.66230622343522649, 0.47314564399410192])\n",
      "(1.0609541173391692, [0.24642012799448015, 0.49094047912050492, 0.66230622343522649, 0.47314564399410192])\n"
     ]
    }
   ],
   "source": [
    "err,DW = implementation2(X,T,W,B,layers.Tanh); print(err,map(numpy.linalg.norm,DW))\n",
    "err,DW = implementation3(X,T,W,B);             print(err,map(numpy.linalg.norm,DW))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now would like to modify the neural network architecture by including a shortcut connection. Shortcut connections can be useful when the prediction requires a combination of simple and more abstract features.\n",
    "\n",
    "![](files/net-shortcut.svg.png)\n",
    "\n",
    "The implementation below assumes two types of nodes, `Sum` and `BranchOut`. Both are needed to implement the network above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implementation3B(X,T,W,B,Sum,BranchOut):\n",
    "    \n",
    "    # 1. Build the neural network\n",
    "    W,B = utils.getparameters([X.shape[1],10,15,10,T.shape[1]])\n",
    "\n",
    "    nodeX = nodes.Input()\n",
    "    nodesW = [nodes.Weight(w) for w in W]\n",
    "    nodesB = [nodes.Bias(b) for b in B]\n",
    "\n",
    "    nodeZ1 = nodes.Linear(nodeX,nodesW[0],nodesB[0])\n",
    "    nodeA1 = nodes.Tanh(nodeZ1)\n",
    "    nodeQ1 = BranchOut(nodeA1)\n",
    "\n",
    "    nodeZ2 = nodes.Linear(nodeQ1,nodesW[1],nodesB[1])\n",
    "    nodeA2 = nodes.Tanh(nodeZ2)\n",
    "    nodeZ3 = nodes.Linear(nodeA2,nodesW[2],nodesB[2])\n",
    "    nodeA3 = nodes.Tanh(nodeZ3)\n",
    "    nodeS3 = Sum([nodeQ1,nodeA3])\n",
    "\n",
    "    nodeZ4 = nodes.Linear(nodeS3,nodesW[3],nodesB[3])\n",
    "    nodeOut = nodes.Output(nodeZ4)\n",
    "\n",
    "    # 2. Compute the error and its gradient\n",
    "    nodeX.feed(X)\n",
    "    Y = nodeOut.evaluate()\n",
    "    err = ((Y-T)**2).mean()\n",
    "    nodeOut.feed(2*(Y-T))\n",
    "    \n",
    "    # 3. Return them\n",
    "    return err,[nodeW.grad() for nodeW in nodesW]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks:**\n",
    "\n",
    " * **Create the nodes `Sum` and `BranchOut` needed for implementing the new architecture (i.e. define two new classes, and implement for each class the required methods).**\n",
    " \n",
    " * **Run the code below to display the error and gradient information for the dataset and current parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.1922440307543112, [0.52149785222097844, 0.85734586001317348, 1.4576722381826273, 1.6381730369377567])\n"
     ]
    }
   ],
   "source": [
    "class Sum:\n",
    "\n",
    "    def __init__(self,I): self.I = I; map(lambda i: i.set_output(self), I); self.reset();\n",
    "\n",
    "    def set_output(self,O): self.O = O\n",
    "\n",
    "    def reset(self): self.o,self.di = None,None\n",
    "\n",
    "    def grad(self):\n",
    "        if self.di is None: self.di = self.O.grad()\n",
    "        return self.di\n",
    "\n",
    "    def evaluate(self):\n",
    "        if self.o is None: self.o = sum(map(lambda i: i.evaluate(), self.I) )\n",
    "        return self.o\n",
    "\n",
    "class BranchOut:\n",
    "\n",
    "    def __init__(self,I): self.I = I; self.O = []; I.set_output(self); self.reset()\n",
    "\n",
    "    def set_output(self,O): self.O.append(O)\n",
    "\n",
    "    def reset(self): self.o,self.di = None,None\n",
    "\n",
    "    def grad(self):\n",
    "        if self.di is None: self.di = sum(map(lambda i: i.grad(), self.O) )\n",
    "        return self.di\n",
    "\n",
    "    def evaluate(self):\n",
    "        if self.o is None: self.o = self.I.evaluate()\n",
    "        return self.o\n",
    "\n",
    "\n",
    "\n",
    "err,DW = implementation3B(X,T,W,B,Sum,BranchOut)\n",
    "print(err,map(numpy.linalg.norm,DW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
