{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks: Forward and Backward Propagation\n",
    "\n",
    "In this homework, our goal is to test different approaches to program neural networks, from the simplest and least general one to the most complex and general. Here, we will be focusing on programming forward and gradient computations. Training neural networks will be left for Homework 2. The first implementation we consider is a hard-coded neural network made of one input layer, three hidden layers, and one output layer. We use the ReLU nonlinearity at each hidden layer. The neural network is depicted below:\n",
    "\n",
    "![](files/net.svg.png)\n",
    "\n",
    "The following implementation performs forward and gradient computations using **`for`**  loops. The code is compact but only works for this specific architecture. It is hard to extend it to new layers without significantly refactoring and complexifying the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy,numpy.linalg\n",
    "\n",
    "def implementation1(X,T,W,B):\n",
    "\n",
    "    # 1. Initialize some data structures\n",
    "    A,Z = [X],[]\n",
    "    DW,DB = [],[]\n",
    "\n",
    "    # 2. Run the forward pass\n",
    "    for i,(w,b) in enumerate(zip(W,B)):\n",
    "        Z.append(A[-1].dot(w)+b)\n",
    "        if i in [0,1,2]: A.append(numpy.maximum(0,Z[-1]))\n",
    "    Y = Z[-1]\n",
    "\n",
    "    # 3. Compute the error\n",
    "    Y = Z[-1]\n",
    "    err = ((Y-T)**2).mean()\n",
    "    grad = 2*(Y-T)\n",
    "\n",
    "    # 4. Gradient propagation\n",
    "    for w,b,a in zip(W[::-1],B[::-1],A[::-1]):\n",
    "        DW.insert(0,a.T.dot(grad)/len(a))\n",
    "        DB.insert(0,grad.mean(axis=0))\n",
    "        grad = grad.dot(w.T)*(a>0)\n",
    "\n",
    "    # 5. Return error and gradient\n",
    "    return err,DW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below applies this function to a given dataset `X,T` at a given position `W,B` in parameter space, where these two variables contain the weight and bias parameters at each layer. It then prints the prediction mean square error, and the gradient norm at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.159390317732545 <map object at 0x000002AEC84D35F8>\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "X,T = utils.getdata(100)\n",
    "W,B = utils.getparameters([X.shape[1],10,15,10,T.shape[1]])\n",
    "\n",
    "err,DW = implementation1(X,T,W,B)\n",
    "\n",
    "print(err,map(numpy.linalg.norm,DW))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers are not specially interesting (the network has not been trained), however, they are useful for debugging purposes, in order to verify that the next implementations work as expected.\n",
    "\n",
    "## Object-Oriented Implementation (15 P)\n",
    "\n",
    "The following implmentation adopts an object oriented approach to the forward and gradient computations. Each layer is an object with methods implementing the forward and backward pass for this layer. Objects are defined in the file `layers.py`. Overall, the code is longer, but it is better structured and easier to extend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import layers\n",
    "    \n",
    "def implementation2(X,T,W,B,NonLin):\n",
    "    \n",
    "    # 1. Build the neural network\n",
    "    nnlayers = []\n",
    "    for w,b in zip(W,B): nnlayers += [layers.Linear(w,b)] + ([NonLin()] if len(b)!=1 else [])\n",
    "    nn = layers.Sequential(nnlayers)\n",
    "\n",
    "    # 2. Compute the error and its gradient\n",
    "    Y = nn.forward(X)\n",
    "    err = ((Y-T)**2).mean()\n",
    "    nn.backward(2*(Y-T))\n",
    "\n",
    "    # 3. Return them\n",
    "    return err,[nn.layers[i].DW for i in [0,2,4,6]]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below computes and prints the prediction error, and the gradient norm at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0609541173391692 <map object at 0x000002AEC84D39E8>\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "err,DW = implementation2(X,T,W,B,layers.Tanh)\n",
    "print(err,map(numpy.linalg.norm,DW))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, although the data and parameters have not been changed, the numbers are different as for `implementation1`, as we have now made use of the `Tanh` nonlinearity instead of `ReLU`.\n",
    "\n",
    "**Tasks:** \n",
    "\n",
    " * **Define a new layer `ReLU` to be used in replacement to the `Tanh` layer. This makes the architecture equivalent to the neural network implemented by the function `implementation1`.**\n",
    " * **Run the code below to verify that the error and gradient are indeed the same for `implementation1` and  `implementation2`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.159390317732545 <map object at 0x000002AECA591630>\n",
      "1.159390317732545 <map object at 0x000002AECA5917F0>\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# TODO: REPLACE BY YOUR OWN CODE\n",
    "# -------------------------------------\n",
    "#import solution; ReLU = solution.ReLU\n",
    "# -------------------------------------\n",
    "class ReLU:\n",
    "    def forward(self,Z): self.A = Z*(Z>0); return self.A\n",
    "    def backward(self,DA): return DA*(1-self.A**2)\n",
    "\n",
    "err,DW = implementation1(X,T,W,B);      print(err,map(numpy.linalg.norm,DW))\n",
    "err,DW = implementation2(X,T,W,B,ReLU); print(err,map(numpy.linalg.norm,DW))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation for General Graphs (15 P)\n",
    "\n",
    "The implementation below is more complex but is also applicable to a broader set of structures than simple feed-forward network. Here, the neural network can be seen as a set of nodes (defined in `nodes.py`) that are organized in a graph. Prediction and computation of gradients are obtained by traversing the graph using recursion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nodes\n",
    "    \n",
    "def implementation3(X,T,W,B):\n",
    "\n",
    "    # 1. Build the neural network\n",
    "    W,B = utils.getparameters([X.shape[1],10,15,10,T.shape[1]])\n",
    "\n",
    "    nodeX = nodes.Input()\n",
    "    nodeA = nodeX\n",
    "    nodesW = [nodes.Weight(w) for w in W]\n",
    "    nodesB = [nodes.Bias(b) for b in B]\n",
    "\n",
    "    for i,(nodeW,nodeB) in enumerate(zip(nodesW,nodesB)):\n",
    "        nodeZ = nodes.Linear(nodeA,nodeW,nodeB)\n",
    "        if i in [0,1,2]: nodeA = nodes.Tanh(nodeZ)\n",
    "    nodeY = nodes.Output(nodeZ)\n",
    "\n",
    "    # 2. Compute the error and its gradient\n",
    "    nodeX.feed(X)\n",
    "    Y = nodeY.evaluate()\n",
    "    err = ((Y-T)**2).mean()\n",
    "    nodeY.feed(2*(Y-T))\n",
    "    \n",
    "    # 3. Return them\n",
    "    return err,[nodeW.grad() for nodeW in nodesW]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below applies the new implementation on the same dataset and parameters as before, and the error and gradients are compared for correctness to those of `implementation2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0609541173391692 <map object at 0x000002AECA59F710>\n",
      "1.0609541173391692 <map object at 0x000002AECA59F710>\n"
     ]
    }
   ],
   "source": [
    "err,DW = implementation2(X,T,W,B,layers.Tanh); print(err,map(numpy.linalg.norm,DW))\n",
    "err,DW = implementation3(X,T,W,B);             print(err,map(numpy.linalg.norm,DW))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now would like to modify the neural network architecture by including a shortcut connection. Shortcut connections can be useful when the prediction requires a combination of simple and more abstract features.\n",
    "\n",
    "![](files/net-shortcut.svg.png)\n",
    "\n",
    "The implementation below assumes two types of nodes, `Sum` and `BranchOut`. Both are needed to implement the network above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implementation3B(X,T,W,B,Sum,BranchOut):\n",
    "    \n",
    "    # 1. Build the neural network\n",
    "    W,B = utils.getparameters([X.shape[1],10,15,10,T.shape[1]])\n",
    "\n",
    "    nodeX = nodes.Input()\n",
    "    nodesW = [nodes.Weight(w) for w in W]\n",
    "    nodesB = [nodes.Bias(b) for b in B]\n",
    "\n",
    "    nodeZ1 = nodes.Linear(nodeX,nodesW[0],nodesB[0])\n",
    "    nodeA1 = nodes.Tanh(nodeZ1)\n",
    "    nodeQ1 = BranchOut(nodeA1)\n",
    "\n",
    "    nodeZ2 = nodes.Linear(nodeQ1,nodesW[1],nodesB[1])\n",
    "    nodeA2 = nodes.Tanh(nodeZ2)\n",
    "    nodeZ3 = nodes.Linear(nodeA2,nodesW[2],nodesB[2])\n",
    "    nodeA3 = nodes.Tanh(nodeZ3)\n",
    "    nodeS3 = Sum([nodeQ1,nodeA3])\n",
    "\n",
    "    nodeZ4 = nodes.Linear(nodeS3,nodesW[3],nodesB[3])\n",
    "    nodeOut = nodes.Output(nodeZ4)\n",
    "\n",
    "    # 2. Compute the error and its gradient\n",
    "    nodeX.feed(X)\n",
    "    Y = nodeOut.evaluate()\n",
    "    err = ((Y-T)**2).mean()\n",
    "    nodeOut.feed(2*(Y-T))\n",
    "    \n",
    "    # 3. Return them\n",
    "    return err,[nodeW.grad() for nodeW in nodesW]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks:**\n",
    "\n",
    " * **Create the nodes `Sum` and `BranchOut` needed for implementing the new architecture (i.e. define two new classes, and implement for each class the required methods).**\n",
    " \n",
    " * **Run the code below to display the error and gradient information for the dataset and current parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-6d3ee2817755>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimplementation3B\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBranchOut\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-997228bb73bb>\u001b[0m in \u001b[0;36mimplementation3B\u001b[0;34m(X, T, W, B, Sum, BranchOut)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[1;31m# 3. Return them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnodeW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnodeW\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnodesW\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-997228bb73bb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[1;31m# 3. Return them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnodeW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnodeW\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnodesW\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mM:\\github\\dnn\\sheet1\\nodes.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[1;32mdef\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradW\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mBias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mM:\\github\\dnn\\sheet1\\nodes.py\u001b[0m in \u001b[0;36mgradW\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgradW\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdw\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mI\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mI\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mM:\\github\\dnn\\sheet1\\nodes.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdi\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "## --------------------------\n",
    "## REPLACE BY YOUR OWN CODE\n",
    "## --------------------------\n",
    "#import solution\n",
    "#Sum = solution.Sum\n",
    "#BranchOut = solution.BranchOut\n",
    "## --------------------------\n",
    "class BranchOut(nodes.Node):\n",
    "    def __init__(self,I):\n",
    "        self.I = I; I.set_output(self); self.reset()\n",
    "    def reset(self): \n",
    "        self.o,self.di = None,None\n",
    "    def evaluate(self): return self.I.evaluate()\n",
    "    def grad(self): return self.di\n",
    "        \n",
    "class Sum(nodes.Node):\n",
    "    def __init__(self,I):\n",
    "        #assert len(I) == 2, 'only allowing 2 entries in Sum layer'\n",
    "        self.I = I\n",
    "        for i in I:\n",
    "            i.set_output(self)\n",
    "        self.reset()\n",
    "    def reset(self): \n",
    "        self.o,self.di = None,None\n",
    "    def evaluate(self):\n",
    "        if self.o is None: \n",
    "            _tmp = self.I[0].evaluate()\n",
    "            for i in self.I[1:]:\n",
    "                _tmp += i.evaluate()\n",
    "            self.o = _tmp\n",
    "        return self.o\n",
    "    def grad(self):\n",
    "        if self.di is None: \n",
    "            self.di = self.O.grad()\n",
    "        return self.di\n",
    "    \n",
    "\n",
    "\n",
    "err,DW = implementation3B(X,T,W,B,Sum,BranchOut)\n",
    "print(err,map(numpy.linalg.norm,DW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,2,3,4]\n",
    "x[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
